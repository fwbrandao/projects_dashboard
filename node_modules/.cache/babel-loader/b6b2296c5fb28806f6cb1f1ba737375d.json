{"ast":null,"code":"const abstractiveSummaryData = {\n  title: \"Abstractive Summarisation\",\n  description: \"The Bart model was proposed in BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. The pretraining task involves randomly shuffling the order of the original sentences and a novel in-filling scheme, where spans of text are replaced with a single mask token.\",\n  gitHubLink: \"https://github.com/fwbrandao/Abstractive_Summarisation\",\n  literactureLink: \"https://arxiv.org/abs/1910.13461\",\n  infoExpantion: {\n    firstHeader: \"What is Summarization?\",\n    firstText: \"Summarization, is to reduce the size of the document while preserving the meaning, is one of the most researched areas among the Natural Language Processing (NLP) community. Summarization techniques, on the basis of whether the exact sentences are considered as they appear in the original text or new sentences are generated using natural language processing techniques, are categorized into extractive and abstractive techniques. Extractive summarization has been a very extensively researched topic and has reached to its maturity stage. The complexities underlying with the natural language text makes abstractive summarization a difficult and a challenging task. Now the research has shifted towards the Abstractive Summarization.\",\n    secondHeader: \"Abstractive Text Summarization\",\n    secondText: \"Abstractive Text Summarization is the task of generating a short and concise summary that captures the salient ideas of the source text. The generated summaries potentially contain new phrases and sentences that may not appear in the source text.\",\n    thirdHeader: \"Facebook-Bart-CNN-Model\",\n    thirdText: \"BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\"\n  }\n};\nexport default abstractiveSummaryData;","map":{"version":3,"sources":["/Users/nando/data_science_dashboard/data_science_projects/src/components/projects/abstractiveSummarisation/abstractiveSummaryData.js"],"names":["abstractiveSummaryData","title","description","gitHubLink","literactureLink","infoExpantion","firstHeader","firstText","secondHeader","secondText","thirdHeader","thirdText"],"mappings":"AAAA,MAAMA,sBAAsB,GAAG;AAC3BC,EAAAA,KAAK,EAAE,2BADoB;AAE3BC,EAAAA,WAAW,EAAE,kUAFc;AAG3BC,EAAAA,UAAU,EAAE,wDAHe;AAI3BC,EAAAA,eAAe,EAAE,kCAJU;AAK3BC,EAAAA,aAAa,EAAE;AACXC,IAAAA,WAAW,EAAE,wBADF;AAEXC,IAAAA,SAAS,EAAE,iuBAFA;AAGXC,IAAAA,YAAY,EAAE,gCAHH;AAIXC,IAAAA,UAAU,EAAE,wPAJD;AAKXC,IAAAA,WAAW,EAAE,yBALF;AAMXC,IAAAA,SAAS,EAAE;AANA;AALY,CAA/B;AAeA,eAAeX,sBAAf","sourcesContent":["const abstractiveSummaryData = {\n    title: \"Abstractive Summarisation\",\n    description: \"The Bart model was proposed in BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. The pretraining task involves randomly shuffling the order of the original sentences and a novel in-filling scheme, where spans of text are replaced with a single mask token.\",\n    gitHubLink: \"https://github.com/fwbrandao/Abstractive_Summarisation\",\n    literactureLink: \"https://arxiv.org/abs/1910.13461\",\n    infoExpantion: {\n        firstHeader: \"What is Summarization?\",\n        firstText: \"Summarization, is to reduce the size of the document while preserving the meaning, is one of the most researched areas among the Natural Language Processing (NLP) community. Summarization techniques, on the basis of whether the exact sentences are considered as they appear in the original text or new sentences are generated using natural language processing techniques, are categorized into extractive and abstractive techniques. Extractive summarization has been a very extensively researched topic and has reached to its maturity stage. The complexities underlying with the natural language text makes abstractive summarization a difficult and a challenging task. Now the research has shifted towards the Abstractive Summarization.\",\n        secondHeader: \"Abstractive Text Summarization\",\n        secondText: \"Abstractive Text Summarization is the task of generating a short and concise summary that captures the salient ideas of the source text. The generated summaries potentially contain new phrases and sentences that may not appear in the source text.\",\n        thirdHeader: \"Facebook-Bart-CNN-Model\",\n        thirdText: \"BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\",\n    }\n}\n\nexport default abstractiveSummaryData;\n"]},"metadata":{},"sourceType":"module"}